{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbEmanuel/IBM-AI-Engineering-Pytorch-Resnet18/blob/main/4_1_resnet18_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mD1XXagoT82"
      },
      "source": [
        "<a href=\"http://cocl.us/pytorch_link_top?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\">\n",
        "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
        "</a> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58fycQpsoT86"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Peh04SM1oT87"
      },
      "source": [
        "<h1><h1>Pre-trained-Models with PyTorch </h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNS7y8gToT87"
      },
      "source": [
        "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions:\n",
        "\n",
        "<ul>\n",
        "<li>change the output layer</li>\n",
        "<li> train the model</li> \n",
        "<li>  identify  several  misclassified samples</li> \n",
        " </ul>\n",
        "You will take several screenshots of your work and share your notebook. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpMk8Rf3oT88"
      },
      "source": [
        "<h2>Table of Contents</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opOnYFTYoT88"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<ul>\n",
        "    <li><a href=\"https://#download_data\"> Download Data</a></li>\n",
        "    <li><a href=\"https://#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
        "    <li><a href=\"https://#data_class\"> Dataset Class</a></li>\n",
        "    <li><a href=\"https://#Question_1\">Question 1</a></li>\n",
        "    <li><a href=\"https://#Question_2\">Question 2</a></li>\n",
        "    <li><a href=\"https://#Question_3\">Question 3</a></li>\n",
        "</ul>\n",
        "<p>Estimated Time Needed: <strong>120 min</strong></p>\n",
        " </div>\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1fY33IeoT89"
      },
      "source": [
        "<h2 id=\"download_data\">Download Data</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hgXddX_oT89"
      },
      "source": [
        "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sKH5q0HoT8-",
        "outputId": "7393d473-46b9-464d-9326-5bd7f595b755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-05 21:22:48--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2598656062 (2.4G) [application/zip]\n",
            "Saving to: ‘Positive_tensors.zip’\n",
            "\n",
            "Positive_tensors.zi 100%[===================>]   2.42G  38.9MB/s    in 72s     \n",
            "\n",
            "2022-12-05 21:24:01 (34.2 MB/s) - ‘Positive_tensors.zip’ saved [2598656062/2598656062]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "942vdvsyoT9A"
      },
      "outputs": [],
      "source": [
        "!unzip -q Positive_tensors.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KK6i-406oT9A",
        "outputId": "827cf1a3-60c6-495b-ea93-03f0399dd148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-05 21:27:18--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2111408108 (2.0G) [application/zip]\n",
            "Saving to: ‘Negative_tensors.zip’\n",
            "\n",
            "Negative_tensors.zi 100%[===================>]   1.97G  31.9MB/s    in 61s     \n",
            "\n",
            "2022-12-05 21:28:20 (32.8 MB/s) - ‘Negative_tensors.zip’ saved [2111408108/2111408108]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
        "!unzip -q Negative_tensors.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3hKAvU8oT9B"
      },
      "source": [
        "We will install torchvision:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm1Vr6eyoT9B",
        "outputId": "105c824e-cbc3-4035-b99d-86311eec953d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hzd3bRHoT9B"
      },
      "source": [
        "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4swq7RvoT9C"
      },
      "source": [
        "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn9gANS1oT9C",
        "outputId": "b8b1d67f-a860-477c-b437-ca6fb502ad46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f15a8b208d0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# These are the libraries will be used for this lab.\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import pandas\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch \n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "import os\n",
        "import glob\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-dUA6xoZoT9C"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pylab as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/Positive_tensors"
      ],
      "metadata": {
        "id": "PgY20mJFs_xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kUgKtNroT9D"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbmjkxfXoT9D"
      },
      "source": [
        "<h2 id=\"data_class\">Dataset Class</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMlTHbDuoT9D"
      },
      "source": [
        "This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svZIYeh6oT9E",
        "outputId": "3fddcbae-553e-4e24-e8e2-86969f4e6726"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "# Create your own dataset object\n",
        "\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,transform=None,train=True):\n",
        "        directory=\"/content/\"\n",
        "        positive=\"Positive_tensors\"\n",
        "        negative='Negative_tensors'\n",
        "\n",
        "        positive_file_path=os.path.join(directory,positive)\n",
        "        negative_file_path=os.path.join(directory,negative)\n",
        "        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
        "        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
        "        number_of_samples=len(positive_files)+len(negative_files)\n",
        "        self.all_files=[None]*number_of_samples\n",
        "        self.all_files[::2]=positive_files\n",
        "        self.all_files[1::2]=negative_files \n",
        "        # The transform is goint to be used on image\n",
        "        self.transform = transform\n",
        "        #torch.LongTensor\n",
        "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
        "        self.Y[::2]=1\n",
        "        self.Y[1::2]=0\n",
        "        \n",
        "        if train:\n",
        "            self.all_files=self.all_files[0:30000]\n",
        "            self.Y=self.Y[0:30000]\n",
        "            self.len=len(self.all_files)\n",
        "        else:\n",
        "            self.all_files=self.all_files[30000:]\n",
        "            self.Y=self.Y[30000:]\n",
        "            self.len=len(self.all_files)     \n",
        "       \n",
        "    # Get the length\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    # Getter\n",
        "    def __getitem__(self, idx):\n",
        "               \n",
        "        image=torch.load(self.all_files[idx])\n",
        "        y=self.Y[idx]\n",
        "                  \n",
        "        # If there is any transform method, apply it onto the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y\n",
        "    \n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK9sXmwxoT9E"
      },
      "source": [
        "We create two dataset objects, one for the training data and one for the validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Z3LLjJjloT9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9df66e9-3092-4f46-83fa-b4780734f663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Dataset(train=True)\n",
        "validation_dataset = Dataset(train=False)\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdTbB8wboT9F"
      },
      "source": [
        "<h2 id=\"Question_1\">Question 1</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ahXr0c2oT9F"
      },
      "source": [
        "<b>Prepare a pre-trained resnet18 model :</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51spx0vqoT9F"
      },
      "source": [
        "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_XD-JSu6oT9G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "04171cc2cb2147328934cc80867bc9b0",
            "3822fdf52ede46dca6e17bd44c4432b3",
            "6e034757e6b14306b74cff736609b727",
            "0b16712e870f47c9b9c26ae726328698",
            "9469c2254e984afa9c27594435c58c99",
            "967f9337e8264954ac75c0269eefead9",
            "66c14fa4b3be4e20a41bfd73cd62b1a1",
            "c116d2b710dd411b9a46612b26a71b43",
            "3714a74a58834045af19472629d48c0e",
            "b0870aceb8fd4f36806feef660f40d48",
            "608182b820a84eb5a11a1e5df049b7e2"
          ]
        },
        "outputId": "b9c39165-b06a-4a3b-eda6-7e2cad564dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04171cc2cb2147328934cc80867bc9b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Step 1: Load the pre-trained model resnet18\n",
        "model = models.resnet18(pretrained=True)\n",
        "# Type your code here\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLcyK7DvoT9G"
      },
      "source": [
        "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jjaq9bkDoT9G"
      },
      "outputs": [],
      "source": [
        "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
        "# Type your code here\n",
        "for param in model.parameters():\n",
        "    param.require_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fD6p8yEoT9H"
      },
      "source": [
        "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85iGllW7oT9H"
      },
      "source": [
        "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2SGNveRWoT9I"
      },
      "outputs": [],
      "source": [
        "# Dimension of last layer\n",
        "d_hidden = 512\n",
        "d_out = 2\n",
        "\n",
        "# replace the output layer\n",
        "model.fc = nn.Linear(d_hidden, d_out)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hgIP8MHoT9J"
      },
      "source": [
        "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uI78zbfnoT9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6ab79f-83b2-4541-a672-75f91910b0c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9jHqMgwoT9N"
      },
      "source": [
        "<h2 id=\"Question_2\">Question 2: Train the Model</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utILfnXQoT9O"
      },
      "source": [
        "In this question you will train your, model:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW7BqjPJoT9P"
      },
      "source": [
        "<b>Step 1</b>: Create a cross entropy criterion function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "54H5VZ2_oT9P"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create the loss function\n",
        "# Type your code here\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh5_97-zoT9Q"
      },
      "source": [
        "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PNjblUs2oT9R"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFEJYu-SoT9R"
      },
      "source": [
        "<b>Step 3</b>: Use the following optimizer to minimize the loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uX-p6iBUoT9R"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjUHd8hZoT9S"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go02dpZNoT9S"
      },
      "source": [
        "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LnkzVXo1oT9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ee14a6-0927-43d4-8185-79a86a44d110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - accuracy: 0.996\n",
            "Process finished!\n"
          ]
        }
      ],
      "source": [
        "n_epochs=1\n",
        "loss_list=[]\n",
        "accuracy_list=[]\n",
        "correct=0\n",
        "N_test=len(validation_dataset)\n",
        "N_train=len(train_dataset)\n",
        "start_time = time.time()\n",
        "#n_epochs\n",
        "\n",
        "Loss=0\n",
        "start_time = time.time()\n",
        "for epoch in range(n_epochs):\n",
        "    for x, y in train_loader:\n",
        "\n",
        "        model.train() \n",
        "        #clear gradient \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #make a prediction \n",
        "        y_pred = model(x)\n",
        "   \n",
        "        # calculate loss \n",
        "        loss = criterion(y_pred, y)\n",
        "    \n",
        "        # calculate gradients of parameters\n",
        "        loss.backward() \n",
        "        \n",
        "        # update parameters \n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_list.append(loss.data)\n",
        "\n",
        "    correct=0\n",
        "    for x_test, y_test in validation_loader:\n",
        "        # set model to eval \n",
        "        model.eval()\n",
        "       \n",
        "        #make a prediction\n",
        "        y_pred = model(x_test) \n",
        "        \n",
        "        #find max \n",
        "        _, yhat = torch.max(y_pred.data, 1)\n",
        "       \n",
        "       \n",
        "        #Calculate misclassified  samples in mini-batch \n",
        "        correct +=(yhat==y_test).sum().item()\n",
        "        \n",
        "   \n",
        "    accuracy=correct/N_test\n",
        "    print(\"Epoch %d - accuracy: %.3f\" % (epoch+1, accuracy))\n",
        "    accuracy_list.append(accuracy)\n",
        "\n",
        "print('Process finished!')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDf76VDGoT9T"
      },
      "source": [
        "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HhdeQy8woT9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a739739-0149-4b9f-bd1a-0ca40cffe064"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9963"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "54j-tfwooT9U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "2b4cd6b0-a3c6-4c73-8e57-0b061b3f175e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(loss_list)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBn0VRFpoT9U"
      },
      "source": [
        "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MsQE2vdoT9V"
      },
      "source": [
        "<b>Identify the first four misclassified samples using the validation data:</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pfUfZG-ToT9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9691ded7-7088-4274-915c-cadd01b3f857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected Label: tensor([1]); Obtained Label: tensor([0])\n",
            "Expected Label: tensor([0]); Obtained Label: tensor([1])\n",
            "Expected Label: tensor([0]); Obtained Label: tensor([1])\n",
            "Expected Label: tensor([0]); Obtained Label: tensor([1])\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "max_num_of_items = 4  # first four mis-classified samples\n",
        "validation_loader_batch_one = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=1)\n",
        "\n",
        "for x_test, y_test in validation_loader_batch_one:\n",
        "    # set model to eval\n",
        "    model.eval()\n",
        "    \n",
        "    # make a prediction\n",
        "    z = model(x_test)\n",
        "    \n",
        "    # find max\n",
        "    _, yhat = torch.max(z.data, 1)\n",
        "    \n",
        "    # print mis-classified samples\n",
        "    if yhat != y_test:\n",
        "        print(\"Expected Label: {}; Obtained Label: {}\".format(str(y_test), str(yhat)))\n",
        "        count += 1\n",
        "        if count >= max_num_of_items:\n",
        "            break\n",
        "    # end if\n",
        "# end for   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDPZ6HCdoT9V"
      },
      "source": [
        "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\"> CLICK HERE </a> Click here to see how to share your notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ4j76U0oT9V"
      },
      "source": [
        "<h2>About the Authors:</h2> \n",
        "\n",
        "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhe7tqXuoT9V"
      },
      "source": [
        "## Change Log\n",
        "\n",
        "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
        "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
        "| 2020-09-21        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n",
        "\n",
        "<hr>\n",
        "\n",
        "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XtIuE6MoT9W"
      },
      "source": [
        "Copyright © 2018 <a href=\"https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\">MIT License</a>.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04171cc2cb2147328934cc80867bc9b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3822fdf52ede46dca6e17bd44c4432b3",
              "IPY_MODEL_6e034757e6b14306b74cff736609b727",
              "IPY_MODEL_0b16712e870f47c9b9c26ae726328698"
            ],
            "layout": "IPY_MODEL_9469c2254e984afa9c27594435c58c99"
          }
        },
        "3822fdf52ede46dca6e17bd44c4432b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_967f9337e8264954ac75c0269eefead9",
            "placeholder": "​",
            "style": "IPY_MODEL_66c14fa4b3be4e20a41bfd73cd62b1a1",
            "value": "100%"
          }
        },
        "6e034757e6b14306b74cff736609b727": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c116d2b710dd411b9a46612b26a71b43",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3714a74a58834045af19472629d48c0e",
            "value": 46830571
          }
        },
        "0b16712e870f47c9b9c26ae726328698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0870aceb8fd4f36806feef660f40d48",
            "placeholder": "​",
            "style": "IPY_MODEL_608182b820a84eb5a11a1e5df049b7e2",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 57.7MB/s]"
          }
        },
        "9469c2254e984afa9c27594435c58c99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "967f9337e8264954ac75c0269eefead9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66c14fa4b3be4e20a41bfd73cd62b1a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c116d2b710dd411b9a46612b26a71b43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3714a74a58834045af19472629d48c0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b0870aceb8fd4f36806feef660f40d48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "608182b820a84eb5a11a1e5df049b7e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}